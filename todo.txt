just future directions 

Threads, blocks, warps, memory hierarchy (shared, global, etc.)

Profiling with nvprof / Nsight

Writing fast vector/matrix ops

Memory alignment + coalescing

Build Your Mini Tensor Library
Use CUDA C++ and PyBind11 to create:

Tensor: basic n-dimensional array w/ shape, strides

add, matmul, dot, sigmoid, softmax kernels

Python wrapper API (tensor = gnp.array([[1,2],[3,4]]))

You’ll be 1 step away from your own ML engine. This will:

Train you for ML frameworks

Test memory, performance, abstraction

Let you reuse this in ray-tracing (you’ll see why below)

Start with GPU Path Tracer
Build a simple real-time ray tracer in CUDA (can run in console or render to file):

Shoot rays from camera

Intersect with spheres/planes

Shade with simple lighting (Lambert, reflection)

Add acceleration structures (BVH) later

This will teach:

Memory layout for scenes

Efficient branching & traversal (big for ML too)

Recursive logic on GPU

This directly links to Ray Tracing + ML. ML accelerators now use ray-style traversal for sparse data too (like sparse attention in Transformers).

4. Learn About Tensor Cores + ML-Specific Acceleration
Later down the line:

Dive into TensorRT, cuDNN, or CUTLASS

Learn how real-world ML gets fused ops and dynamic kernels

Start optimizing your mini ML library with those

5. Start Plugging Graphics + ML
Super cool mashups you can explore:

Idea	Combines
Neural Radiance Fields (NeRF)	Ray tracing + ML
Real-time style transfer on GPU	Graphics + ML
GAN-powered texture generation	Math + ML + graphics
Physics sim + training agents	Math + RL
